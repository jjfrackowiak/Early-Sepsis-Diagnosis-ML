{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Evaluating NeuralNets\n",
    "\n",
    "Sections:\n",
    "- Imports\n",
    "- Data exploration\n",
    "- Dataloader\n",
    "- NN Architectures\n",
    "- Lightning wrapper for architectures\n",
    "- Hyperparameters tuning\n",
    "    - Preparing dataloader and pruner\n",
    "    - LSTM grid-search\n",
    "    - TCN grid-search\n",
    "    - ANN grid-search\n",
    "- Examining results (logs) of hyperparams tuning\n",
    "- Training models with best hyperparams to visualise results\n",
    "    - Dataloader\n",
    "    - Plot loss function\n",
    "    - LSTM (Best model)\n",
    "    - TCN (Best hparams)\n",
    "    - ANN (Best hparams)\n",
    "- Shap values for the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0OH2js_2ifT"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1703242959795,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "d4nOyU6A2ifV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional.regression import symmetric_mean_absolute_percentage_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from packaging import version\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, balanced_accuracy_score\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Scipy\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sn\n",
    "sn.set_theme(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import plotly.figure_factory as ff\n",
    "import shap\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNAJyEkT2ifX"
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSZfCcHj2ifZ",
    "outputId": "e79c1a70-6011-4019-960a-b193019a1bd7"
   },
   "outputs": [],
   "source": [
    "# Column indicating patients\n",
    "sepsis_df.groupby('File_Path').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA-3C4Fr2ifZ",
    "outputId": "a6e155dd-934f-4d56-e7b4-d7b5787fce72"
   },
   "outputs": [],
   "source": [
    "# Plotting the pandas describe table\n",
    "sepsis_df.describe().map(\"{0:.4f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGaSCXVb2ifa",
    "outputId": "4123c52e-9610-49c2-f4f5-80bff2b44152"
   },
   "outputs": [],
   "source": [
    "sepsis_df.isna().sum()/sepsis_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcaT1W492ifa",
    "outputId": "6b4d7343-64d4-4459-c466-e92f91409207"
   },
   "outputs": [],
   "source": [
    "# Possibly the distribution of missing data is skewed\n",
    "sepsis_df[sepsis_df['SepsisLabel']==1].isna().sum()/sepsis_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pZLugqD2ifa"
   },
   "outputs": [],
   "source": [
    "# Group by 'File_Path' and check if there is at least one 'SepsisLabel' equal to 1 in each group\n",
    "grouped = sepsis_df.groupby('File_Path')['SepsisLabel'].any().reset_index()\n",
    "\n",
    "# Identify the groups where 'SepsisLabel' is equal to 1\n",
    "groups_with_sepsis = grouped[grouped['SepsisLabel'] == 1]['File_Path'].tolist()\n",
    "\n",
    "# Update the entire group to have 'SepsisLabel' equal to 1\n",
    "sepsis_df.loc[sepsis_df['File_Path'].isin(groups_with_sepsis), 'SepsisLabel'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepsis_df.to_csv('all_sample_data_target_recode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IzeW1M52ifb",
    "outputId": "4aaa5e43-9d3e-4118-9dfb-0e70da945650"
   },
   "outputs": [],
   "source": [
    "# Is the distribution of missing data skewed?\n",
    "sepsis_df[sepsis_df['SepsisLabel']==1].isna().sum()/sepsis_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1PEuKOt2ifb",
    "outputId": "d83b92dd-2b1d-4942-f7c7-4c788b6f3ac2"
   },
   "outputs": [],
   "source": [
    "# Possibly the distribution of missing data skewed? (No)\n",
    "sepsis_df[sepsis_df['SepsisLabel']==0].isna().sum()/sepsis_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny9S7kQ32ifb",
    "outputId": "36c796f7-0355-414a-c22e-67d4120aedc7"
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'SepsisLabel' is equal to 0\n",
    "filtered_df = sepsis_df[sepsis_df['SepsisLabel'] == 0]\n",
    "\n",
    "# Group by 'File_Path' for the filtered DataFrame\n",
    "grouped_df = filtered_df.groupby('File_Path')\n",
    "\n",
    "# Identify groups with at least one column having all values as null\n",
    "groups_with_all_nulls = []\n",
    "\n",
    "for group_name, group_data in grouped_df:\n",
    "    if group_data.isnull().all(axis=0).any():\n",
    "        groups_with_all_nulls.append(group_name)\n",
    "\n",
    "# Calculate the percentage of groups with at least one column having all values as null\n",
    "percentage_groups_with_all_nulls = (len(groups_with_all_nulls) / len(grouped_df)) * 100\n",
    "\n",
    "print(f\"Percentage of groups with at least one column having all values as null: {percentage_groups_with_all_nulls:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gey2AzcR2ifc",
    "outputId": "567cface-42f7-4c53-c81f-10fa3b0590da"
   },
   "outputs": [],
   "source": [
    "# We don't have any extreme/unnatural observations (Although we have statistical outliers)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'sepsis_df' is DataFrame\n",
    "# You may want to exclude non-numeric columns if they are not suitable for histograms\n",
    "numeric_columns = sepsis_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Determine the number of rows and columns dynamically based on the number of numeric columns\n",
    "num_plots = len(numeric_columns)\n",
    "num_cols = min(4, num_plots)  # Maximum of 3 columns per row\n",
    "num_rows = -(-num_plots // num_cols)  # Ceiling division to calculate the number of rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 5 * num_rows))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Flatten the axes array if it's a 2D array\n",
    "axes = axes.flatten() if num_rows > 1 else [axes]\n",
    "\n",
    "# Plot histograms for each numeric column with density\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    sns.histplot(sepsis_df[column], kde=True,  bins=40,  stat='percent', ax=axes[i])\n",
    "    axes[i].set_title(f'Histogram of {column}')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for j in range(num_plots, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyLPb_uA2ifc"
   },
   "outputs": [],
   "source": [
    "sepsis_df.to_csv('sepsis_target_recode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1703245986132,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "Cx8EQAl42ifd"
   },
   "outputs": [],
   "source": [
    "# Custom lightning dataloader\n",
    "class SepsisDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, train_dir, seq_len, X_scaler, X_imputer,\n",
    "                 test_size, random_state, batch_size, num_workers, device):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.seq_len = seq_len\n",
    "        self.X_scaler = X_scaler\n",
    "        self.X_imputer = X_imputer\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.sepsis_train = None\n",
    "        self.sepsis_val = None\n",
    "        self.sepsis_test = None\n",
    "\n",
    "    def setup(self, stage):\n",
    "        pass\n",
    "\n",
    "    def prepare_for_training(self):\n",
    "        # Load the dataset\n",
    "        self.train_df = pd.read_csv(self.train_dir)\n",
    "\n",
    "        # Create X and y\n",
    "        self.y = self.train_df[[\"SepsisLabel\"]].values\n",
    "        self.X = self.train_df.drop([\"HospAdmTime\"], axis=1)\n",
    "\n",
    "\n",
    "        # Adding interactions for columns with less <70% missing values\n",
    "        self.X['hr_o2sat_interaction'] = self.X['HR'] * self.X['O2Sat']\n",
    "        self.X['hr_sbp_interaction'] = self.X['HR'] * self.X['SBP']\n",
    "        self.X['mpa_dbp_interaction'] = self.X['MAP'] * self.X['DBP']\n",
    "        self.X['o2sat_resp_interaction'] = self.X['O2Sat'] * self.X['Resp']\n",
    "        self.X['temp_resp_interaction'] = self.X['Temp'] * self.X['Resp']\n",
    "        self.X['age_resp_interaction'] = self.X['Age'] * self.X['Resp']\n",
    "\n",
    "        # Adding 1diffs for columns with less <70% missing values\n",
    "        variables_to_diff = ['Temp', 'DBP', 'Resp', 'SBP', 'O2Sat', 'MAP', 'HR']\n",
    "\n",
    "        # Calculate first differences for the specified variables grouped by 'Patient_id'\n",
    "        for variable in variables_to_diff:\n",
    "            self.X[f'{variable}_diff'] = self.X.groupby('File_Path')[variable].diff()\n",
    "\n",
    "        # Getting patients:\n",
    "        # unique_patient_ids = self.X['File_Path'].unique()\n",
    "\n",
    "        # Separate patients with sepsis_label=1 and sepsis_label=0\n",
    "        sepsis_label_1_ids = self.X[self.X['SepsisLabel'] == 1]['File_Path'].unique()\n",
    "        sepsis_label_0_ids = self.X[self.X['SepsisLabel'] == 0]['File_Path'].unique()\n",
    "\n",
    "        # Split sepsis_label=1 patients\n",
    "        train_sepsis_label_1_ids, temp_sepsis_label_1_ids = train_test_split(sepsis_label_1_ids, test_size=self.test_size, random_state=self.random_state)\n",
    "        val_sepsis_label_1_ids, test_sepsis_label_1_ids = train_test_split(temp_sepsis_label_1_ids, test_size=0.5, random_state=self.random_state)\n",
    "\n",
    "        # Split sepsis_label=0 patients\n",
    "        train_sepsis_label_0_ids, temp_sepsis_label_0_ids = train_test_split(sepsis_label_0_ids, test_size=self.test_size, random_state=self.random_state)\n",
    "        val_sepsis_label_0_ids, test_sepsis_label_0_ids = train_test_split(temp_sepsis_label_0_ids, test_size=0.5, random_state=self.random_state)\n",
    "\n",
    "        # Combine the splits for both sepsis_label=1 and sepsis_label=0\n",
    "        train_patient_ids = list(train_sepsis_label_1_ids) + list(train_sepsis_label_0_ids)\n",
    "        val_patient_ids = list(val_sepsis_label_1_ids) + list(val_sepsis_label_0_ids)\n",
    "        test_patient_ids = list(test_sepsis_label_1_ids) + list(test_sepsis_label_0_ids)\n",
    "\n",
    "        # Filter the data for training, validation, and test sets based on patient_id\n",
    "        train_mask = self.X['File_Path'].isin(train_patient_ids)\n",
    "        val_mask = self.X['File_Path'].isin(val_patient_ids)\n",
    "        test_mask = self.X['File_Path'].isin(test_patient_ids)\n",
    "\n",
    "        # Drop the label and duplicated index\n",
    "        self.X = self.X.drop([\"SepsisLabel\", \"Unnamed: 0\"], axis=1)\n",
    "\n",
    "        self.X_train = self.X[train_mask]\n",
    "        self.X_val = self.X[val_mask]\n",
    "        self.X_test = self.X[test_mask]\n",
    "\n",
    "        self.y_train = self.y[train_mask]\n",
    "        self.y_val = self.y[val_mask]\n",
    "        self.y_test = self.y[test_mask]\n",
    "\n",
    "        # Omit the 'File_path' column\n",
    "        self.columns_to_keep = [col for col in self.X_train.columns if col not in ['File_Path']]\n",
    "\n",
    "        # If imputers are provided, they're applied to the data\n",
    "        if self.X_imputer:\n",
    "            # Flatten and scale X_train\n",
    "            self.X_train[self.columns_to_keep] = self.X_imputer.fit_transform(\n",
    "                self.X_train[self.columns_to_keep])\n",
    "\n",
    "            # Scale X_val and y_val\n",
    "            self.X_val[self.columns_to_keep] = self.X_imputer.transform(\n",
    "                self.X_val[self.columns_to_keep])\n",
    "\n",
    "            # Scale X_test and y_test\n",
    "            self.X_test[self.columns_to_keep] = self.X_imputer.transform(\n",
    "                self.X_test[self.columns_to_keep])\n",
    "\n",
    "        # If scalers are provided, they're applied to the data\n",
    "        if self.X_scaler:\n",
    "            # Flatten and scale X_train\n",
    "            self.X_train[self.columns_to_keep] = self.X_scaler.fit_transform(\n",
    "                self.X_train[self.columns_to_keep])\n",
    "\n",
    "            # Scale X_val and y_val\n",
    "            self.X_val[self.columns_to_keep] = self.X_scaler.transform(\n",
    "                self.X_val[self.columns_to_keep])\n",
    "\n",
    "            # Scale X_test and y_test\n",
    "            self.X_test[self.columns_to_keep] = self.X_scaler.transform(\n",
    "                self.X_test[self.columns_to_keep])\n",
    "        \n",
    "        # \n",
    "        # Generating sliding windows\n",
    "        self.X_train, self.y_train = self.create_sequences(self.X_train, self.y_train, self.seq_len)\n",
    "        self.X_val, self.y_val = self.create_sequences(self.X_val, self.y_val, self.seq_len)\n",
    "        self.X_test, self.y_test = self.create_sequences(self.X_test, self.y_test, self.seq_len)\n",
    "\n",
    "\n",
    "        # Create three instances of the custom dataset class\n",
    "        self.sepsis_train = SepsisData(self.X_train,\n",
    "                                       self.y_train,\n",
    "                                       device=self.device\n",
    "                                             )\n",
    "        self.sepsis_val = SepsisData(self.X_val,\n",
    "                                     self.y_val,\n",
    "                                     device=self.device\n",
    "                                             )\n",
    "        self.sepsis_test = SepsisData(self.X_test,\n",
    "                                      self.y_test,\n",
    "                                      device=self.device\n",
    "                                            )\n",
    "\n",
    "    def create_sequences(self, X, y, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "\n",
    "        for patient_id, group_X in X.groupby('File_Path'):\n",
    "            data_X = group_X.drop('File_Path', axis=1).to_numpy()\n",
    "            data_y = y[X.index.isin(group_X.index)]\n",
    "\n",
    "            num_obs = len(data_X)\n",
    "\n",
    "            # Calculate the number of windows for the patient\n",
    "            num_windows = max(1, num_obs - seq_length + 1)\n",
    "\n",
    "            # Zero-pad if there are not enough observations\n",
    "            if num_obs < seq_length:\n",
    "                padded_data_X = np.pad(data_X, ((0, seq_length - num_obs), (0, 0)),\n",
    "                                    mode='constant', constant_values=0)\n",
    "                sequences.append(torch.tensor(padded_data_X, dtype=torch.float32))\n",
    "                targets.append(torch.tensor(data_y[-1], dtype=torch.float32))\n",
    "            else:\n",
    "                for i in range(num_windows):\n",
    "                    window_X = data_X[i:i + seq_length, :]\n",
    "                    window_y = data_y[i + seq_length - 1]\n",
    "                    sequences.append(torch.tensor(window_X, dtype=torch.float32))\n",
    "                    targets.append(torch.tensor(window_y, dtype=torch.float32))\n",
    "        return torch.stack(sequences), torch.stack(targets)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return the dataloader of train data\n",
    "        return DataLoader(self.sepsis_train,\n",
    "                          shuffle=True,\n",
    "                          batch_size=self.batch_size,\n",
    "                          drop_last=False,\n",
    "                          num_workers=self.num_workers,\n",
    "                          persistent_workers=True\n",
    "                          )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return the dataloader of validation data\n",
    "        return DataLoader(self.sepsis_val,\n",
    "                          shuffle=False,\n",
    "                          batch_size=self.batch_size,\n",
    "                          drop_last=False,\n",
    "                          num_workers=self.num_workers,\n",
    "                          persistent_workers=True\n",
    "                          )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # Return the dataloader of test data\n",
    "        return DataLoader(self.sepsis_test,\n",
    "                          shuffle=False,\n",
    "                          batch_size=self.batch_size,\n",
    "                          drop_last=False,\n",
    "                          num_workers=self.num_workers,\n",
    "                          persistent_workers=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKylD4Zf2ifd"
   },
   "source": [
    "### NN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1703241850230,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "07MmtzWM2ifd"
   },
   "outputs": [],
   "source": [
    "# LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, in_size, h_size, num_l, out_f, dropout_rate, activ):\n",
    "        super().__init__()\n",
    "        self.input_size = in_size\n",
    "        self.hidden_size = h_size\n",
    "        self.num_layers = num_l\n",
    "        self.out_features = out_f\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activ = activ\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=self.dropout_rate\n",
    "                            )\n",
    "        self.linear1 = nn.Linear(in_features=self.hidden_size,\n",
    "                                 out_features=self.out_features)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X, _ = self.lstm(X)\n",
    "        # Picking the last output\n",
    "        X = self.activ(X[:,-1,:])\n",
    "        X = self.linear1(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1703241850731,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "qt_cFKtO2ifd"
   },
   "outputs": [],
   "source": [
    "class TemporalCNNModel(nn.Module):\n",
    "    def __init__(self, in_channels=10,\n",
    "                 out_channels=10,\n",
    "                 kernel_size=3,\n",
    "                 dropout_rate=0.2,\n",
    "                 output_size=1,\n",
    "                 num_conv_layers=3):\n",
    "        super(TemporalCNNModel, self).__init__()\n",
    "\n",
    "        # Define a list to hold the convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        # Add the first convolutional layer\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size))\n",
    "        self.conv_layers.append(nn.ReLU())\n",
    "        self.conv_layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Add additional convolutional layers\n",
    "        for _ in range(1, num_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv1d(out_channels, out_channels, kernel_size))\n",
    "            self.conv_layers.append(nn.ReLU())\n",
    "            self.conv_layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(out_channels, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.permute(0, 2, 1) # X was prepared for LSTM\n",
    "        for layer in self.conv_layers:\n",
    "            X = layer(X)\n",
    "\n",
    "        X = self.pool(X)\n",
    "        X = X.view(X.size(0), -1)  # Flatten for fully connected layer\n",
    "        X = self.fc(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1703241850990,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "zpiCzuDZ2ife"
   },
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, in_size=15,\n",
    "                 h_sizes=[60,30,15],\n",
    "                 out_size=1,\n",
    "                 dropout_rate=0.2,\n",
    "                 activ=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.input_size = in_size\n",
    "        self.hidden_sizes = h_sizes\n",
    "        self.output_size = out_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activ = activ()\n",
    "\n",
    "        # Create a list to hold the fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "\n",
    "        # Add input layer\n",
    "        self.fc_layers.append(nn.Linear(in_features=self.input_size, out_features=self.hidden_sizes[0]))\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(self.hidden_sizes)):\n",
    "            self.fc_layers.append(nn.Linear(in_features=self.hidden_sizes[i-1], out_features=self.hidden_sizes[i]))\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_layers.append(nn.Linear(in_features=self.hidden_sizes[-1], out_features=self.output_size))\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.fc_layers[:-1]:\n",
    "            X = self.activ(layer(X))\n",
    "            X = self.dropout(X)\n",
    "\n",
    "        # Output layer without activation function\n",
    "        X = self.fc_layers[-1](X)[:,-1,:]\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R7D1zNL2ife"
   },
   "source": [
    "### Lightning wrapper for architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1703245065041,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "UxLnIPG42ife"
   },
   "outputs": [],
   "source": [
    "# Wrapper for models created with lightning\n",
    "class ModelWrapped(pl.LightningModule):\n",
    "    def __init__(self, model, lr=0.01):\n",
    "        super().__init__()\n",
    "        # Model of choice\n",
    "        self.model = model\n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_targets = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        self.train_losses = []\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        y_pred = self.model(X).squeeze()\n",
    "        pos_weight = torch.tensor(8)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        self.log(\"train_loss\", train_loss, prog_bar=True, on_step=True, on_epoch=False, sync_dist=True)\n",
    "\n",
    "        self.predictions.extend(y_pred.unsqueeze(0))\n",
    "        self.targets.extend(y.unsqueeze(0))\n",
    "        self.train_losses.append(train_loss)\n",
    "\n",
    "        return train_loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        all_predictions =  torch.cat(self.predictions, dim=0).detach().cpu().numpy()\n",
    "        all_targets = torch.cat(self.targets, dim=0).detach().cpu().numpy()\n",
    "\n",
    "        # Calculate average train loss\n",
    "        avg_train_loss = torch.stack(self.train_losses).mean()\n",
    "\n",
    "        # Calculate average validation ROC_AUC\n",
    "        roc_auc = roc_auc_score(all_targets, all_predictions)\n",
    "\n",
    "        self.log(\"train_epoch_loss\", avg_train_loss.item(), prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"train_epoch_roc_auc\", roc_auc, prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        self.train_losses.clear()\n",
    "        self.predictions.clear()\n",
    "        self.targets.clear()\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        X, y = batch\n",
    "        y_pred = self.model(X).squeeze()\n",
    "        pos_weight = torch.tensor(8)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        val_loss = criterion(y_pred, y)\n",
    "        \n",
    "        self.val_step_outputs.extend(y_pred.unsqueeze(0))\n",
    "        self.val_step_targets.extend(y.unsqueeze(0))\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, on_step=True, on_epoch=False, sync_dist=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if len(self.val_step_outputs) > 2000:\n",
    "            val_all_outputs =  torch.cat(self.val_step_outputs, dim=0).detach().cpu().numpy()\n",
    "            val_all_targets = torch.cat(self.val_step_targets, dim=0).detach().cpu().numpy()\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = torch.stack(self.val_losses).mean()\n",
    "            \n",
    "            # Calculate average validation ROC_AUC\n",
    "            roc_auc = roc_auc_score(val_all_targets, val_all_outputs)\n",
    "\n",
    "            self.log(\"val_epoch_loss\", avg_val_loss.item(), prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "            self.log(\"val_epoch_roc_auc\", roc_auc, prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "            self.val_losses.clear()\n",
    "            self.val_step_outputs.clear()\n",
    "            self.val_step_targets.clear()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        with torch.no_grad():\n",
    "            X, y = batch\n",
    "            y_pred = self.model(X).squeeze()\n",
    "        return y_pred\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08O8oVOM2ife"
   },
   "source": [
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r-eLyxu2ife"
   },
   "source": [
    "#### Preparing Dataloader and Pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 135303,
     "status": "ok",
     "timestamp": 1703246135516,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "HVp7Qgrh2ife"
   },
   "outputs": [],
   "source": [
    "import dataloader_wrapper_cuda\n",
    "import importlib\n",
    "\n",
    "importlib.reload(dataloader_wrapper_cuda)\n",
    "\n",
    "# Pruner\n",
    "class OptunaPruning(PyTorchLightningPruningCallback, pl.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "# Load the scaler from a pickle file\n",
    "scaler_filename = os.path.join('scalers_imputers_for_sample_all', 'min_max_scaler_all_feat.pkl')\n",
    "with open(scaler_filename, 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "# Load the imputer from a pickle file\n",
    "imputer_filename = os.path.join('scalers_imputers_for_sample_all', 'bayesian_imputer_all_feat.pkl')\n",
    "with open(imputer_filename, 'rb') as file:\n",
    "    imputer = pickle.load(file)\n",
    "\n",
    "\n",
    "dm = dataloader_wrapper_cuda.SepsisDataLoader(train_dir=\"data/all/all_sample_data_target_recode.csv\",\n",
    "                          seq_len=20,\n",
    "                          X_scaler=scaler,\n",
    "                          X_imputer=imputer,\n",
    "                          test_size=0.3,\n",
    "                          random_state=42,\n",
    "                          batch_size=256,\n",
    "                          training_size=1,\n",
    "                          num_workers=6,\n",
    "                          device=device\n",
    "                          )\n",
    "dm.prepare_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model to a file using pickle\n",
    "model_filename = os.path.join('scalers_imputers_for_sample_all', 'min_max_scaler_all_feat.pkl')\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(dm.X_scaler, file)\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "model_filename = os.path.join('scalers_imputers_for_sample_all', 'bayesian_imputer_all_feat.pkl')\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(dm.X_imputer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP5lBBT12iff"
   },
   "source": [
    "#### LSTM grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVA-MY5W2iff"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    h_size = trial.suggest_categorical('h_size',[16, 32, 48])\n",
    "    num_l = trial.suggest_categorical('num_l',[2])\n",
    "    dropout_rate = trial.suggest_categorical('dropout_rate',[0.2, 0.4])\n",
    "    activ = nn.ReLU\n",
    "    lr = trial.suggest_categorical('lr', [1e-3, 1e-2])\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    dropout = trial.suggest_categorical('dropout', [0.2])\n",
    "\n",
    "    torch_model = LSTMModel(in_size=51,\n",
    "                                h_size=h_size,\n",
    "                                num_l=num_l,\n",
    "                                out_f=1,\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                activ=activ()).to(device)\n",
    "    model = ModelWrapped(torch_model,\n",
    "                         lr=lr).to(device)\n",
    "\n",
    "    callback = OptunaPruning(trial, monitor=\"val_loss\")\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        default_root_dir='logs_20_window_all_data/lstm_logs/',\n",
    "        #limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=10,\n",
    "        accelerator=\"gpu\",\n",
    "        callbacks=[callback],        \n",
    "    )\n",
    "    hyperparameters = dict(seq_len = 20,\n",
    "                            h_size = h_size,\n",
    "                            num_l = num_l,\n",
    "                            dropout_rate = dropout_rate,\n",
    "                            activ = activ,\n",
    "                            lr = lr,\n",
    "                            batch_size = 32\n",
    "                            )\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    pl.seed_everything(0)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "    callback.check_pruned()\n",
    "    \n",
    "    # Free up memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return trainer.callback_metrics[\"val_epoch_roc_auc\"].item()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "#torch.set_float32_matmul_precision('medium')\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "study.optimize(objective, n_trials=12)\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    h_size = trial.suggest_categorical('h_size',[16, 32, 48])\n",
    "    num_l = trial.suggest_categorical('num_l',[4])\n",
    "    dropout_rate = trial.suggest_categorical('dropout_rate',[0.2, 0.4])\n",
    "    activ = nn.ReLU\n",
    "    lr = trial.suggest_categorical('lr', [1e-3, 1e-2])\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    dropout = trial.suggest_categorical('dropout', [0.2])\n",
    "\n",
    "    torch_model = LSTMModel(in_size=51,\n",
    "                                h_size=h_size,\n",
    "                                num_l=num_l,\n",
    "                                out_f=1,\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                activ=activ()).to(device)\n",
    "    model = ModelWrapped(torch_model,\n",
    "                         lr=lr)\n",
    "\n",
    "    callback = OptunaPruning(trial, monitor=\"val_loss\")\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        default_root_dir='logs_20_window_all_data/lstm_logs',\n",
    "        #limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=10,\n",
    "        accelerator=\"gpu\",\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "    hyperparameters = dict(seq_len = 20,\n",
    "                            h_size = h_size,\n",
    "                            num_l = num_l,\n",
    "                            dropout_rate = dropout_rate,\n",
    "                            activ = activ,\n",
    "                            lr = lr,\n",
    "                            batch_size = 32\n",
    "                            )\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    pl.seed_everything(0)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "    callback.check_pruned()\n",
    "    return trainer.callback_metrics[\"val_epoch_roc_auc\"].item()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "study.optimize(objective, n_trials=12)\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbQ3ZaWk2iff"
   },
   "source": [
    "#### TCN grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712,
     "referenced_widgets": [
      "ae13c70fa3d74e979563e2de98719640",
      "b2fca7c80a9741f8b6f34716a20fe8ce",
      "87d420babfde4901bcac57bf893bd5d3",
      "b015ece45d744eeb9bcb148788f752e1",
      "86776ec661b64dd3a7921ed3ac3033b5",
      "9e99e11de59a4d7687fdf2204f8318e6",
      "8da6a065024a40ffa43d27785da6faa9",
      "f87eb74de4f14c039f20ce80efbd92bb",
      "d7ca6a16603b4389813edffcad193fed",
      "d2f9f65b77b94214ba419f75d875455e",
      "1bc14a8f72a2476d8e2b0b650a126cea",
      "8386d0b0abde49dbac65095550cf85df",
      "757858e501be4a428d949c2cbca26232",
      "b988df8cd97e4e4f830d31dd17f10185",
      "121a239a10e746f4b4d2e8194cb48778",
      "ac0ed306cc214725a37751e7c34192cc",
      "b7da395f74fc4b4c8fca7c74ae79533c",
      "77b9c3a7fe4d4aa6a37412110275288b",
      "5c29b6ed57d34884a0bfa2d1b04c16e6",
      "9d2d03d577ad465b951272ce57039486",
      "ad9df6ce2f534952827953f492c068a3",
      "5e59a74f8bb246ecb5517eca886b4991",
      "eba71deb71684a3e80160ae958cdb0a0",
      "af364ee2969148e693804d6981dab991",
      "bb9879f53b164921a7732e6d1cc0e93a",
      "141ed90088b04dbdb59757b2169ac196",
      "2a8b5e2fdd4e4faea4cd3ea31c2f8026",
      "863c0084b2c3478d8c1227df850f4bca",
      "0469c9c440c1456d9380c4fddd7f656d",
      "9d3d40f1c6934561b4c124bec217e465",
      "037cc460deed4ef7a7f8108a1e46b880",
      "c13ddd82742d4852bb060e4258a91123",
      "08a83ad50d084856ae048f775062fec1",
      "c093930de2a147b48e24cb7ff43b828c",
      "66f230c38a9345659d0fd25662b4bfbc",
      "8bedbb97b2e449ee8e105ab64c753be0",
      "dd7a989c0c35402885fc0d67ee0394ba",
      "e6724f7ee7d04a02a255efda32abf2cb",
      "ba4d68022f2a4b52af059a657116d3af",
      "5b22872fa1d24397bdb54d83cbf4d581",
      "44606da14dc948f1b86566ff87479b73",
      "aaab2f2e4fd44f239658cd3cc421b47b",
      "d5adff0685034c39ac8b1594a23b8520",
      "b6daa17f0de3430b890395905e8b6990",
      "982c4c1eb11f4ca69c4243aba893ad1e",
      "c0416ff7b8124f25bc0aac7d911ab20e",
      "87071c5187cc46eab953ede5068e4338",
      "3f7452c6c3874d73a7f1ba5741b1d7df",
      "99a4f5e77c6d41fb933975bb105167d9",
      "320838ae13464d10a9df1839a13ddbd6",
      "b7e1d7b9768b41d28699a26a48cb2c2b",
      "77ea2af099484539afa2a3211364dc14",
      "c5cafd7370ff49579573319a1b80ee30",
      "c8750095db1e41e594cf3eb18e685b8c",
      "abde4035566e477cb1e54aa79af57f41",
      "b58e3c4dac6244faa089c61056dd9cba",
      "00176da570424b818b75ac64e8395338",
      "9fb60731161b4ae5b1ab4c5778404fa8",
      "51aa1e5b6a8f45c0818e4188dbf7ddc1",
      "49a35fc3a2a44083b3c1def99a1a8c39",
      "0a50a1b779544e2880aca9ef218545e8",
      "37717b0c8945480492882bdea502b7e6",
      "d108ad20feb64ca9857bfb283d6d1144",
      "f2af48f79c3e4a3db392e938b40b4112",
      "990646a4a8674d6fae1cf0affe841e45",
      "80311284a7d04ebf87df68b8b6081134",
      "646b0db71f1a4058bb6b08443357a5f4",
      "6f845df568b145af973289c8657930c3",
      "ced1b59e38ca424b839e342a9453018b",
      "8422246dc5ea42cca8a00aa1124d42fb",
      "66407595c42a4735a6cabf06714a49bb",
      "96cce93311d9430695847f83ee82a232",
      "85eb194ff01c41ef92d2ba70fd4e6fbb",
      "558022c008b04ba59a5cae264c16a92e",
      "8bf11c3a929d49b4818e8803c5aeef24",
      "33f0d3bd40da4744b17784b5430cf31e",
      "b170a21aa8e6478bb19c43bebd228a54",
      "4db3ae395d2a48f9925955b208b81229",
      "d11e5bb93ddd459f804fa24f4e6e1323",
      "91e91a6914074972bf4424775dc84121",
      "848d2aadfaa64d64896f3aced8c11ffb",
      "e50035469b3145e4811ce5510855a23f",
      "53fc4d4ea07849f8b5b6754d38bdb17a",
      "e26e4d88cabb4567887317c053bdc11b",
      "3604f000b8b44b3598077973b4e5b2bf",
      "aa7d6442c4304a11a02f06cd12160a02",
      "195674c5ca834501ade82e701ed2fd41",
      "9eea540ca8014f8aa7a60b2a0459a647",
      "eaca6bdae2d543a1ae1dc3e72d7edab5",
      "c1e37b4276144f61b48b4bd105a27162",
      "313f369c949a46c891af797b693c5717",
      "bfe2f04d58354bb3b388198587e790c1",
      "40cff1d9fa4e48dda4decf6cbbf96db7",
      "507c00cd5c2f4d95b3ea5b7a8d835412",
      "d0c62924e9d44916a333b2baf0bd3113",
      "f8a25816b78042f7b510dceddc512187",
      "a3aa2a560d3242c59954b0aea0ffce6f",
      "78d84a0e24614a8da1cd3c236dbedd53",
      "41f1e4dce0c1491598534bde7b699fd3",
      "31c3bade781e4f7bb8af38c80af85ebf",
      "ab2cc02ff5884f298e4ca277b3056fb5",
      "64a5d70f0f8541ec9bd96a4596cfff3b",
      "e713d25ae20349a7a82bd5204a5549ff",
      "59ac405aa4784b6899a8fc88ae7bcfef",
      "2e008630545043d1972aab691f63b652",
      "72ffcd59d4dd49fbbf5e732863081c30",
      "d0b743133e2c42f3b2a6adf6214fe445",
      "a40a9048539c40189293a8effa75d8bb",
      "6c18ccfd61804416b35538cf23301951",
      "a3d5af757f01446fbb858f2ff1d13507",
      "8ef2c29fc1a147698ddc73e3009c9298",
      "3bcaa238e8cb4ca19952179bd8bcf393",
      "0f737723cbab4e66ab0aa4a139659577",
      "671e930abde24c9aa8333d8b889b0ebc",
      "02365f2aa3fc42f6b406a6818a2a2699",
      "8edc246d191a4f7a8cced116097ae119",
      "41087ff7e032449588eae4100de79c64",
      "e6a94faef9d4442aaef18c0aa8ab68f5",
      "b25d11cebb1c4d4b87beec3272221b97",
      "42d67594515f4f0eb5421cb0277362e1",
      "5c700184d63649baba18643acf95b3d9",
      "1379ec1416c447dc8f2b348399bbef9e",
      "4e95527ac2f741559a80c38d734b640b",
      "3f71a6fc4ce6425db526b5a766203ae9",
      "78abc945af964cb8b64533f626d4ce19",
      "fddf46a819b149d390b4773167fb9da5",
      "82970d95d3b246668dd53263f6b79f8f",
      "0a6a930910a243b9a1df7c57141c175d",
      "b7683434a390445bbff47c5de4aca22b",
      "16de6e6cb97d4f68aa701c7377aa87a0",
      "eb7c9b7812004bf0bbe49bed149e4616",
      "117a7e927e144d4d8b52de14969e89a6"
     ]
    },
    "executionInfo": {
     "elapsed": 659687,
     "status": "ok",
     "timestamp": 1703206766098,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "ypdWY-lu2iff",
    "outputId": "ca3ae48e-cb11-4c66-9cab-8e823f8fa02a"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    seq_len = trial.suggest_categorical('seq_len',[20])\n",
    "    num_tcn_channels  = trial.suggest_categorical('num_tcn_channels',[15,\n",
    "                                            20, 40])\n",
    "    dropout_rate = trial.suggest_categorical('dropout_rate',[0.2])\n",
    "    activ = nn.ReLU\n",
    "    lr = trial.suggest_categorical('lr',[1e-3, 1e-2])\n",
    "    batch_size = 32\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    n_layers = trial.suggest_categorical('n_layers', [2, 4])\n",
    "    dropout = trial.suggest_categorical('dropout', [0.2, 0.4])\n",
    "\n",
    "    torch_model = TemporalCNNModel(in_channels=51,\n",
    "                 out_channels=num_tcn_channels,\n",
    "                 kernel_size=3,\n",
    "                 dropout_rate=0.2,\n",
    "                 output_size=1,\n",
    "                 num_conv_layers=n_layers).to(device)\n",
    "    model = ModelWrapped(torch_model,\n",
    "                         lr=lr)\n",
    "\n",
    "    callback = OptunaPruning(trial, monitor=\"val_loss\")\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        default_root_dir='logs_20_window_all_data/tcn_logs/',\n",
    "        #limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=10,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "    hyperparameters = dict(seq_len = seq_len,\n",
    "                            num_hidden_channels  = num_tcn_channels,\n",
    "                            dropout_rate = dropout_rate,\n",
    "                            activ = activ,\n",
    "                            lr = lr,\n",
    "                            batch_size = batch_size,\n",
    "                            n_layers = n_layers,\n",
    "                            dropout = dropout)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    pl.seed_everything(0)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "    callback.check_pruned()\n",
    "    return trainer.callback_metrics[\"val_epoch_roc_auc\"].item()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "study.optimize(objective, n_trials=24)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cq7lp892iff"
   },
   "source": [
    "#### ANN grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712,
     "referenced_widgets": [
      "eb0c8aa9726949988be40d04f09ea0cb",
      "d66887fbc74147abb7e50826ac4c2616",
      "a76b340599d7409da4db5666e1c7a04e",
      "d0caf3d5b0b04e98896dcfe55f5850e9",
      "2f68a187731c47e5a9fabe71122b4dd3",
      "e3c86b755cba43e88e9daaf716c9bbda",
      "557add9127374037b3169f0e73b461fd",
      "3338d74a8bf24cd888b7d1875273e703",
      "385630a85df0438381222918f2dd9977",
      "bec0366dc4854aa18d9c08c25ac4e2fc",
      "7cee2e62b7a14481a5f4f76f50ca2e0e",
      "a52504cfa34446308e6ebaeac73c7edb",
      "1f92958d4f5843bd9fe7746b5d45a94d",
      "14bf21fa1b1f41ec8d3831e29414b1e8",
      "09aaa8b78c26431999804b113accb976",
      "f6ded6f76ba04ab7b3f27a7983f42dd8",
      "5b99712b0d554ca9b35c31c3cba91e83",
      "38bd9fa5bb82420bbe514cb33a469222",
      "480fe960a7cd4efc915b6ba87dc1b6ec",
      "741857501892410492b3cfb0a14dbc09",
      "335c1af308cb46faa4bf931453c6bca1",
      "f533960093934d799b6e4e62657be22b",
      "652a4d256d45432f827b127cf221bbf3",
      "54f185472060403d9617ef5074ba4e05",
      "e08d467e8c2642e4a8b936950cd41c26",
      "eb0ba9c85dec4aa0a6e334324e4a30eb",
      "035e033da9684ea5abbac732b4e7c182",
      "fc05f5c2f4f943f389b3d6f74741359d",
      "a644e193e0c7467480e1be1246bdc94d",
      "b14aedb3819a49e88809e015af718d5a",
      "675822c9fe27407cb365af97a9ca4611",
      "caba15842c4f4f3aa41d0faa743e4c6d",
      "3b37c04cef614450b0a6ac2b121e645f",
      "5f0cc4ea319948138317c786eb5c6c33",
      "a3be6e97dac643278eb1cb309b7b9263",
      "3a6aa26d4ea044fcb93eef8f2ba1f917",
      "44062b81445945feb808dd131945949b",
      "e80543bcc269420abd6e035921269625",
      "a90dd8ad4fbb44b58059d57151236aae",
      "816150949a7143448e424b2997a4cfaa",
      "63c86ad0f3d045b9aabcc97382250f23",
      "7ae9f4dd44c94679b44e4ec232fd05e7",
      "29ca80b3418c410eb7a0c58f4ee6af71",
      "f1b85b2768bd4cd68415d46c2f7a4437",
      "561a45ca238d4f7ca0bf788d37b6fbd5",
      "ba9df4a6f54b4741b9c1272b3ba578c3",
      "5eefea4ffabc4067a2a0a9435c7df4b2",
      "11abb4348e9f46e886d6aaac4663d246",
      "04107e7d51df4add9e406a464dd75db1",
      "e35e4a330c5341e5a3d259860809e7eb",
      "a2c5c2ca1718419f8dd18c138efc62fc",
      "d8c91f705ad541e7a5c417abdc4c5224",
      "81e33ecb2e3d43d4a83a7f5192b0fdc5",
      "63ac43698bbe418096e787e81b92d349",
      "74a13385c3264a0e809a147bd2bdd18b",
      "c68e954ce6e84272aa9ffa156a5cb886",
      "b6f0d9b2b34d4d7bb766b4b57f1263d3",
      "64430be5c04d413ea65f024696aa49b3",
      "e1c72d853cc140869bf6e07fb2020d45",
      "ea1ed6f26f9a4e91be4af6db2a718a51",
      "4db75befdfff456783f6d8d52d575fd5",
      "5822f31fdcbb497098e32f329ca3c978",
      "9d0ed9baf99641bbb5e640622d9a7e6d",
      "03578f4f25a4464eb2ff89c241f663ee",
      "de715a03920147c592d57bc987336a16",
      "ea73a29e08204bc5a7863facf35c15a0",
      "f8eb976311b1495191aa0f377205183e",
      "086a8428c8d944778bd62f79bb6d2546",
      "658343549d06424d8dda5fc48a269e1f",
      "1f6a8d6b1dfc4bc1abe179097a60dd80",
      "31dbd8221e4d4a47a8a8bcf6f27f6db8",
      "0f1000f40e214af5b64afe12abc4fde0",
      "87b857fe77ee4bc5a4a0fe87b55e803a",
      "136b4994d03a47f49a18c69f9daf81c0",
      "163f78fa23c24f32ab94f31f2ae60140",
      "77cce0205f61464d9f6d745e2ce3b2ed",
      "48a7cca48a75448b86ec8f62eec52ca7",
      "a9ebd99604cd487c8a49b27e3807d305",
      "067f6a9810424f328404be05fa96165b",
      "796f4b1ccf824fd3a8f5173b0f465398",
      "1f9d6dbbc4774770a86403d6c97a7b2d",
      "0424820e187d469089eaf81214288676",
      "18016dd0e84446bfa521f845e29387bd",
      "2dd2378fed3f4f1b8f2b00736b61e92c",
      "9eb911594dd64f229875a87a043f191a",
      "9f97a15fb8f7498d9b4cfeb080542f1b",
      "81ac772b096e457796f276095d18c59e",
      "1691b984c08d4979ac613bfd8006272d",
      "e1eb126814c8436ca5421dbcf1b114d7",
      "0d9611aaf4f24261b19763af9db7afcd",
      "5a99dcb76d1f4c57a8875596cee59bdf",
      "bf2c0f5818234c9f86d49f16a2d93744",
      "6eff8211bc074d719b9df7fdf47e4010",
      "c26050c6c57d4d23bb1da2df36e44367",
      "16149e2bd3544ec8a812b48030d50dfa",
      "0aa861e57a934253a6ca448ddf6986cc",
      "6989e5fb608f489a8e5fba87c7ce6073",
      "328b1ecae8b14bea89ecddf5c2402bd7",
      "3636499f05d544f3a81ee76e55cf7356",
      "ece5e3481c914d1fba1b4ba93e88054d",
      "b184f53ebef24a8cbf39910e48fe69a2",
      "a243fca3dac94cac97f77c71b3841eed",
      "a7ed19cc2af94824aecb6305ac20c28d",
      "aac8e92c18cd49ab9f2915f39a645ca3",
      "28100b0ad30a4751ab9c2c53c3952e0b",
      "dd94de5b812f4ac7b9cc40fa7a0de707",
      "69da75015652454abd7effebd352f4fb",
      "cea1263e7ae243f89d85561d8b5fffc7",
      "8bb03c4cf4bc4afd8381bd6bf44a98c4",
      "41a100b05a9a4df48196d519a3b958d0",
      "fd5d838dcb8f4f7d8a151c07ef02eda8",
      "1f990567d962413fa3bc9f39af18d151",
      "4f3b0e88cbba40b1b0fec3eee379ca1a",
      "334d98368a25479c9014e0efa593f355",
      "d1f4a9434e5e4e7bbda60b051af66474",
      "5e341c1eaecb49b4914f9092ab056841",
      "f4eefc2ad9884fd0877c8441d332e175",
      "601840238f0244cb94cc2da1e74b78fc",
      "ac40548e17e343a7bb1276238fc63afa",
      "e03aac869d3642909ef755a5a58d3343",
      "da626f28d6f7432a9bc51c94fe923ec2",
      "1be1193fc80c46dcb13a89be9cbf0542",
      "3a8c0f4dd22045adbde98ecebfc7e7ad",
      "6903b85f67ec4516a61dc8524d4a181d",
      "29f813352f9b4f9ea0fa3cb26b2258cc",
      "8be20a071bc84fae9bd07673f73a36a5",
      "cb2b069550874cf7b72ad557b63fcbe8",
      "64c04551ddc24325a31b649edc9d0b16",
      "304b9241cce74734af666fc0739cdf78",
      "08c212fd3e7f4238bcabdcafca59eaf4",
      "0b4d87631f394c949847b9273023c59d",
      "c727173270044c21a2f58d9777b5d237"
     ]
    },
    "executionInfo": {
     "elapsed": 637046,
     "status": "ok",
     "timestamp": 1703207403141,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "TkW88M8k2iff",
    "outputId": "37386c13-55db-4d4b-b21b-d2d16b4c526d"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    seq_len = trial.suggest_categorical('seq_len',[20])\n",
    "    h_size = trial.suggest_categorical('h_size',[16, 32])\n",
    "    ann_hidden_sizes = trial.suggest_categorical('ann_hidden_size',[[60,30,15],\n",
    "                                            [80,40,20]])\n",
    "    dropout_rate = trial.suggest_categorical('dropout_rate',[0.2, 0.4])\n",
    "    activ = nn.ReLU\n",
    "    lr = trial.suggest_categorical('lr',[1e-3, 1e-2])\n",
    "    batch_size = 32\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "\n",
    "    torch_model = ANNModel(in_size=51,\n",
    "                            h_sizes=ann_hidden_sizes,\n",
    "                            out_size=1,\n",
    "                            dropout_rate=dropout_rate,\n",
    "                            activ=activ).to(device)\n",
    "    model = ModelWrapped(torch_model,\n",
    "                         lr=lr)\n",
    "\n",
    "    callback = OptunaPruning(trial, monitor=\"val_loss\")\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        default_root_dir='logs_20_window_all_data/ann_logs/',\n",
    "        #limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=10,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "    hyperparameters = dict(seq_len = seq_len,\n",
    "                            h_size = h_size,\n",
    "                            ann_hidden_sizes = ann_hidden_sizes,\n",
    "                            dropout_rate = dropout_rate,\n",
    "                            activ = activ,\n",
    "                            lr = lr,\n",
    "                            batch_size = batch_size)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    pl.seed_everything(0)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "    callback.check_pruned()\n",
    "    return trainer.callback_metrics[\"val_epoch_roc_auc\"].item()\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "study.optimize(objective, n_trials=16)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining results of hyperparams tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### window_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "\n",
    "# Define the root directory containing lstm_logs, tcn_logs, and ann_logs\n",
    "root_dir = os.path.join(os.getcwd(), 'logs_15_window')\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "columns = ['Model', 'Hyperparameters', 'Last Train Loss', 'Last Val Loss']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Iterate through model directories (lstm, tcn, ann)\n",
    "for model in ['lstm', 'tcn', 'ann']:\n",
    "    model_dir = os.path.join(root_dir, f'{model}_logs/lightning_logs')\n",
    "    \n",
    "    # Iterate through version directories\n",
    "    for version_dir in os.listdir(model_dir):\n",
    "        version_path = os.path.join(model_dir, version_dir)\n",
    "\n",
    "        # Check if the directory is valid (contains the necessary files)\n",
    "        if os.path.isdir(version_path):\n",
    "            events_file_path = os.path.join(version_path, 'events.out.tfevents.*')\n",
    "            yaml_file_path = os.path.join(version_path, 'hparams.yaml')\n",
    "            \n",
    "            # Find the latest events file\n",
    "            events_files = sorted(glob.glob(events_file_path), key=os.path.getmtime, reverse=True)\n",
    "            if events_files:\n",
    "                latest_events_file = events_files[0]\n",
    "                \n",
    "                # Read the last train_loss and val_loss from the events file\n",
    "                summary_iterator = tf.compat.v1.train.summary_iterator(latest_events_file)\n",
    "                last_train_loss = None\n",
    "                last_val_loss = None\n",
    "                \n",
    "                for event in summary_iterator:\n",
    "                    if event.HasField('summary'):\n",
    "                        for value in event.summary.value:\n",
    "                            if value.tag == 'train_loss':\n",
    "                                last_train_loss = value.simple_value\n",
    "                            elif value.tag == 'val_loss':\n",
    "                                last_val_loss = value.simple_value\n",
    "                            \n",
    "                # Read hyperparameters from the YAML file using FullLoader\n",
    "                with open(yaml_file_path, 'r') as yaml_file:\n",
    "                    hparams = yaml.full_load(yaml_file)\n",
    "                \n",
    "                # Append a new row to the DataFrame\n",
    "                new_row = pd.Series({\n",
    "                    'Model': model,\n",
    "                    'Hyperparameters': hparams,\n",
    "                    'Last Train Loss': last_train_loss,\n",
    "                    'Last Val Loss': last_val_loss\n",
    "                }, name=len(df))  # Using len(df) as the index to ensure unique index values\n",
    "\n",
    "                df = pd.concat([df, new_row.to_frame().T])\n",
    "\n",
    "df.to_csv('tuning_results_15_window.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() #Dropping pruned trials\n",
    "best_trial = df[df['Last Val Loss']==min(df['Last Val Loss'])]\n",
    "print(best_trial['Hyperparameters'].values)\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### window_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "\n",
    "# Define the root directory containing lstm_logs, tcn_logs, and ann_logs\n",
    "root_dir = os.path.join(os.getcwd(), 'logs_20_window_all_data')\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "columns = ['Model', 'Hyperparameters', 'Last Train Loss', 'Last Val Loss', 'Last Train ROC', 'Last Val ROC']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Iterate through model directories (lstm, tcn, ann)\n",
    "for model in ['lstm', 'tcn', 'ann']:\n",
    "    model_dir = os.path.join(root_dir, f'{model}_logs/lightning_logs')\n",
    "    \n",
    "    # Iterate through version directories\n",
    "    for version_dir in os.listdir(model_dir):\n",
    "        version_path = os.path.join(model_dir, version_dir)\n",
    "\n",
    "        # Check if the directory is valid (contains the necessary files)\n",
    "        if os.path.isdir(version_path):\n",
    "            events_file_path = os.path.join(version_path, 'events.out.tfevents.*')\n",
    "            yaml_file_path = os.path.join(version_path, 'hparams.yaml')\n",
    "            \n",
    "            # Find the latest events file\n",
    "            events_files = sorted(glob.glob(events_file_path), key=os.path.getmtime, reverse=True)\n",
    "            if events_files:\n",
    "                latest_events_file = events_files[0]\n",
    "                \n",
    "                # Read the last train_loss and val_loss from the events file\n",
    "                summary_iterator = tf.compat.v1.train.summary_iterator(latest_events_file)\n",
    "                last_train_loss = None\n",
    "                last_val_loss = None\n",
    "                \n",
    "                for event in summary_iterator:\n",
    "                    if event.HasField('summary'):\n",
    "                        for value in event.summary.value:\n",
    "                            if value.tag == 'train_loss':\n",
    "                                last_train_loss = value.simple_value\n",
    "                            elif value.tag == 'val_loss':\n",
    "                                last_val_loss = value.simple_value\n",
    "                            elif value.tag == 'train_epoch_roc_auc':\n",
    "                                last_train_roc = value.simple_value\n",
    "                            elif value.tag == 'val_epoch_roc_auc':\n",
    "                                last_val_roc = value.simple_value\n",
    "                            \n",
    "                # Read hyperparameters from the YAML file using FullLoader\n",
    "                with open(yaml_file_path, 'r') as yaml_file:\n",
    "                    hparams = yaml.full_load(yaml_file)\n",
    "                \n",
    "                # Append a new row to the DataFrame\n",
    "                new_row = pd.Series({\n",
    "                    'Model': model,\n",
    "                    'Hyperparameters': hparams,\n",
    "                    'Last Train Loss': last_train_loss,\n",
    "                    'Last Val Loss': last_val_loss,\n",
    "                    'Last Train ROC': last_train_roc,\n",
    "                    'Last Val ROC': last_val_roc\n",
    "                }, name=len(df))  # Using len(df) as the index to ensure unique index values\n",
    "\n",
    "                df = pd.concat([df, new_row.to_frame().T])\n",
    "\n",
    "df.to_csv('tuning_results_20_window.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() #Dropping pruned trial\n",
    "best_trial = df[df['Last Val ROC']==max(df['Last Val ROC'])]\n",
    "print(best_trial['Hyperparameters'].values)\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df.groupby('Model')['Last Val ROC'].idxmax()]['Hyperparameters'].values)\n",
    "df.loc[df.groupby('Model')['Last Val ROC'].idxmax()]\n",
    "# BATCH IS 256!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO1Gmvgy2ifg"
   },
   "source": [
    "### Training models with best hyperparams to visualise results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_wrapper_cuda import SepsisData, SepsisDataLoader\n",
    "\n",
    "dm = SepsisDataLoader(train_dir=\"data/all/all_sample_data_target_recode.csv\",\n",
    "                          seq_len=20,\n",
    "                          X_scaler=scaler,\n",
    "                          X_imputer=imputer,\n",
    "                          test_size=0.3,\n",
    "                          random_state=42,\n",
    "                          batch_size=32,\n",
    "                          training_size=1,\n",
    "                          num_workers=11,\n",
    "                          device=device\n",
    "                          )\n",
    "dm.prepare_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file using pickle\n",
    "model_filename = os.path.join('scalers_imputers_for_sample_all', 'min_max_scaler')\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(dm.X_scaler, file)\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "model_filename = os.path.join('scalers_imputers_for_sample_all', 'bayesian_imputer')\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(dm.X_imputer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuFZLDkp2ifg"
   },
   "source": [
    "#### Plot loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTp1oLEX2ifg"
   },
   "outputs": [],
   "source": [
    "# Plot the loss curve based on the provided folder or the latest folder from lightning logs\n",
    "def plot_loss_curve(fpath=None, model_name='Model'):\n",
    "    latest_file = os.path.join(fpath, \"metrics.csv\")\n",
    "    loss_data = pd.read_csv(latest_file)\n",
    "    # loss_data['val_loss'] = loss_data.val_loss.shift(1)\n",
    "    loss_data = loss_data[[\"epoch\", 'train_epoch_loss', 'val_epoch_loss', 'val_epoch_roc_auc','train_epoch_roc_auc']]\n",
    "  \n",
    "    loss_data.fillna(method='bfill', inplace=True)\n",
    "    loss_data.fillna(method='ffill', inplace=True)\n",
    "    loss_data['epoch'] = loss_data['epoch'].astype(int)\n",
    "    loss_data = loss_data.groupby(\"epoch\").mean()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(loss_data.index, loss_data['train_epoch_loss'], label=\"Training loss\")\n",
    "    plt.plot(loss_data.index, loss_data['train_epoch_roc_auc'], label=\"Training ROC-AUC\", linestyle='--')\n",
    "    plt.plot(loss_data.index, loss_data['val_epoch_loss'], label=\"Validation loss\")\n",
    "    plt.plot(loss_data.index, loss_data['val_epoch_roc_auc'], label=\"Validation ROC-AUC\", linestyle='--')\n",
    "    plt.title(f\"{model_name} Loss Curve\", fontsize=13)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Metric\", fontsize=12)\n",
    "    plt.xticks(loss_data.index)\n",
    "    plt.legend(loc=\"upper right\", fontsize=10)#, bbox_to_anchor = (1, 0.68))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353,
     "referenced_widgets": [
      "9786ff7dd15b47cdb1b4a19b77fcdc2c",
      "8427cbbd45fb4420a571d995ac529782",
      "a46d234dce224c75a35c805f0ddc3dca",
      "60b9626412934a7fb36dfadc248293fa",
      "efceb13d394748a092a75a2d87dc130d",
      "558070d2b7db4a5fb412d48e5c6e5dd2",
      "e816b1520a5d4adc83b31fdcd25bfdae",
      "aa0242bc843946c1b4b290359753daea",
      "a6972efb2c6e40e9a5eda7f1fd5f3493",
      "da061c118bc948a988f82be638202fd5",
      "1ee49dfdc9b54d7abe428b69c9b45aac",
      "4d1c6836f33e49029ff8064e093ce5a0",
      "27b3b12be042480e82ba62294020eeb3",
      "a1d5231941584fc9b1c94e886cdf7980",
      "36509243f03147608f9b249ef4ce50fc",
      "895b27520c234b5c9deffed17a81645d",
      "d2e60120870e48718156c808a8059f9e",
      "76c7b400c2b44bbc94e7e0796554c27a",
      "ae0de50ec49545c6bfc821142ee90470",
      "a315f84e9e42447d90f637ac296be7c8",
      "6ca5d15eb84548fd9a97f5a581796adb",
      "6a1c71f4a7594ebabab9b4147f016367"
     ]
    },
    "id": "dukZW28i2ifk",
    "outputId": "0ef5cfbf-8d8b-4337-8ec3-f37d5d0e96f6"
   },
   "outputs": [],
   "source": [
    "#torch.set_float32_matmul_precision('medium')\n",
    "# Create an instance of ModelWrapped\n",
    "model = LSTMModel(in_size=51,\n",
    "                  h_size=48,\n",
    "                  num_l=2,\n",
    "                  out_f=1,\n",
    "                  dropout_rate=0.4,\n",
    "                  activ=nn.ReLU()).to(device)  # Replace with actual model\n",
    "model_wrapped = ModelWrapped(model, lr=0.01)\n",
    "logger = pl.loggers.CSVLogger('csv_logs_all')\n",
    "\n",
    "# Initialize the Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        #limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=15,\n",
    "        accelerator=\"gpu\"\n",
    "    )  # Adjust the number of GPUs as needed\n",
    "\n",
    "pl.seed_everything(0)\n",
    "\n",
    "trainer.fit(model_wrapped, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9945,
     "status": "ok",
     "timestamp": 1703245584430,
     "user": {
      "displayName": "Jan Frąckowiak",
      "userId": "04383337243094606279"
     },
     "user_tz": -60
    },
    "id": "OuaW3TLaTJHh",
    "outputId": "315c23c9-c52c-4129-e74d-74929d4d5b39"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "# model_filename = os.path.join('models_nn_all', 'sepsis_lstm.pkl')\n",
    "# with open(model_filename, 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "\n",
    "# Load the model using pickle\n",
    "model_filename = os.path.join('models_nn_all', 'sepsis_lstm.pkl')\n",
    "with open(model_filename, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "\n",
    "# Load test dataset\n",
    "model_wrapped = ModelWrapped(model)\n",
    "val_dataloader = dm.val_dataloader()\n",
    "\n",
    "# List to store target and predicted values\n",
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "model_wrapped = ModelWrapped(model)\n",
    "\n",
    "# Iterate over batches in the test dataloader\n",
    "for idx, batch in enumerate(val_dataloader):\n",
    "    # Assuming targets are in the first element of the batch tuple\n",
    "    batch_targets = batch[1].clone()\n",
    "    targets.extend(batch_targets.cpu().numpy())  # Convert to numpy if not already\n",
    "\n",
    "    # Assuming LightningModule has a predict_step method\n",
    "    # Make sure to move the batch to the same device as the model\n",
    "    batch = [tensor.to('cpu') for tensor in batch]\n",
    "\n",
    "    output = model_wrapped.predict_step(batch)\n",
    "\n",
    "    # Assuming model returns raw logits, apply softmax to get probabilities\n",
    "    probabilities = torch.sigmoid(output)\n",
    "    predictions.extend(probabilities.cpu().detach().numpy())  # Convert to numpy if not already\n",
    "\n",
    "# Convert the lists to PyTorch tensors for convenience\n",
    "targets_tensor = torch.tensor(targets)\n",
    "predictions_tensor = torch.tensor(predictions)\n",
    "\n",
    "# Plot histograms for target and predicted distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(targets_tensor.numpy(), bins='auto', color='blue', alpha=0.7)\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Target Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_tensor.numpy(), bins=20, color='orange', alpha=0.7)\n",
    "plt.title('Prediction Distribution')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the lists to NumPy arrays for convenience\n",
    "targets_np = targets_tensor.numpy()\n",
    "predictions_np = predictions_tensor.numpy()\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr_lstm, tpr_lstm, thresholds = roc_curve(targets_np, predictions_np)\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(targets_np, predictions_np)\n",
    "\n",
    "# Visualize ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_lstm, tpr_lstm, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - LSTM')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Calculate balanced accuracy with a threshold of 0.9\n",
    "threshold = 0.93\n",
    "binary_predictions = (predictions_np > threshold).astype(int)\n",
    "balanced_acc = balanced_accuracy_score(targets_np, binary_predictions)\n",
    "\n",
    "print(f'Balanced Accuracy with threshold {threshold}: {balanced_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp1nTqcV2ifk",
    "outputId": "6069429e-ae96-4d31-d3c4-7a9b6f228ae3"
   },
   "outputs": [],
   "source": [
    "# The model seems to have reached the best performance at around 20th epoch\n",
    "plot_loss_curve(\"csv_logs_all/lightning_logs/version_0\", model_name='LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCN (best hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ModelWrapped\n",
    "model = TemporalCNNModel(in_channels=51,\n",
    "                 out_channels=20,\n",
    "                 kernel_size=3,\n",
    "                 dropout_rate=0.2,\n",
    "                 output_size=1,\n",
    "                 num_conv_layers=4).to(device)  # Replace with actual model\n",
    "model_wrapped = ModelWrapped(model, lr=0.001)\n",
    "val_dataloader = dm.val_dataloader()\n",
    "# Logger type\n",
    "logger = pl.loggers.CSVLogger('csv_logs_all')\n",
    "\n",
    "# Specify the early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # Choose the metric to monitor (e.g., validation loss)\n",
    "    patience=4,           # Number of epochs with no improvement after which training will be stopped\n",
    "    mode='min'            # 'min' means training will stop when the monitored quantity has stopped decreasing\n",
    ")\n",
    "\n",
    "# Initialize the Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        #limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=15,\n",
    "        accelerator=\"gpu\"\n",
    "        #callbacks=[early_stopping_callback]\n",
    "    )  # Adjust the number of GPUs as needed\n",
    "\n",
    "pl.seed_everything(0)\n",
    "trainer.fit(model_wrapped, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Save the model to a file using pickle\n",
    "# model_filename = os.path.join('models_nn_all', 'sepsis_tcn.pkl')\n",
    "# with open(model_filename, 'wb') as file:\n",
    "#      pickle.dump(model, file)\n",
    "     \n",
    "with open('models_nn_all/sepsis_tcn.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load test dataset\n",
    "model_wrapped = ModelWrapped(model)\n",
    "val_dataloader = dm.val_dataloader()\n",
    "\n",
    "# List to store target and predicted values\n",
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "model_wrapped = ModelWrapped(model)\n",
    "\n",
    "# Iterate over batches in the test dataloader\n",
    "for idx, batch in enumerate(val_dataloader):\n",
    "    # Assuming targets are in the first element of the batch tuple\n",
    "    batch_targets = batch[1].clone()\n",
    "    targets.extend(batch_targets.cpu().numpy())  # Convert to numpy if not already\n",
    "\n",
    "    # Assuming LightningModule has a predict_step method\n",
    "    # Make sure to move the batch to the same device as the model\n",
    "    batch = [tensor.to('cpu') for tensor in batch]\n",
    "\n",
    "    output = model_wrapped.predict_step(batch)\n",
    "\n",
    "    # Assuming model returns raw logits, apply softmax to get probabilities\n",
    "    probabilities = torch.sigmoid(output)\n",
    "    predictions.extend(probabilities.cpu().detach().numpy())  # Convert to numpy if not already\n",
    "\n",
    "# Convert the lists to PyTorch tensors for convenience\n",
    "targets_tensor = torch.tensor(targets)\n",
    "predictions_tensor = torch.tensor(predictions)\n",
    "\n",
    "# Plot histograms for target and predicted distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(targets_tensor.numpy(), bins='auto', color='blue', alpha=0.7)\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Target Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_tensor.numpy(), bins=20, color='orange', alpha=0.7)\n",
    "plt.title('Prediction Distribution')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the lists to NumPy arrays for convenience\n",
    "targets_np = targets_tensor.numpy()\n",
    "predictions_np = predictions_tensor.numpy()\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr_tcn, tpr_tcn, thresholds = roc_curve(targets_np, predictions_np)\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(targets_np, predictions_np)\n",
    "\n",
    "# Visualize ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_tcn, tpr_tcn, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - TCN')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Calculate balanced accuracy with a threshold of 0.9\n",
    "threshold = 0.93\n",
    "binary_predictions = (predictions_np > threshold).astype(int)\n",
    "balanced_acc = balanced_accuracy_score(targets_np, binary_predictions)\n",
    "\n",
    "print(f'Balanced Accuracy with threshold {threshold}: {balanced_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model seems to have reached the best performance at around 20th epoch\n",
    "plot_loss_curve(\"csv_logs_all/lightning_logs/version_1\", model_name='TCN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN (best hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ModelWrapped\n",
    "model = ANNModel(in_size=51,\n",
    "                out_size=1,\n",
    "                h_sizes=[80,40,20],\n",
    "                dropout_rate=0.2,\n",
    "                activ=nn.ReLU).to(device)\n",
    "model_wrapped = ModelWrapped(model, lr=0.001)\n",
    "logger = pl.loggers.CSVLogger('csv_logs_all')\n",
    "\n",
    "# Specify the early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # Choose the metric to monitor (e.g., validation loss)\n",
    "    patience=3,           # Number of epochs with no improvement after which training will be stopped\n",
    "    mode='min'            # 'min' means training will stop when the monitored quantity has stopped decreasing\n",
    ")\n",
    "\n",
    "# Initialize the Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        #limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=15,\n",
    "        accelerator=\"gpu\"\n",
    "        #callbacks=[early_stopping_callback]\n",
    "    )  # Adjust the number of GPUs as needed\n",
    "\n",
    "\n",
    "pl.seed_everything(0)\n",
    "trainer.fit(model_wrapped, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file using pickle\n",
    "# model_filename = os.path.join('models_nn_all', 'sepsis_ann.pkl')\n",
    "# with open(model_filename, 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "    \n",
    "\n",
    "# Save the model to a file using pickle\n",
    "model_filename = os.path.join('models_nn_all', 'sepsis_ann.pkl')\n",
    "with open(model_filename, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Load test dataset\n",
    "model_wrapped = ModelWrapped(model)\n",
    "val_dataloader = dm.val_dataloader()\n",
    "\n",
    "# List to store target and predicted values\n",
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "model_wrapped = ModelWrapped(model)\n",
    "\n",
    "# Iterate over batches in the test dataloader\n",
    "for idx, batch in enumerate(val_dataloader):\n",
    "    # Assuming targets are in the first element of the batch tuple\n",
    "    batch_targets = batch[1].clone()\n",
    "    targets.extend(batch_targets.cpu().numpy())  # Convert to numpy if not already\n",
    "\n",
    "    # Assuming LightningModule has a predict_step method\n",
    "    # Make sure to move the batch to the same device as the model\n",
    "    batch = [tensor.to('cpu') for tensor in batch]\n",
    "\n",
    "    output = model_wrapped.predict_step(batch)\n",
    "\n",
    "    # Assuming model returns raw logits, apply softmax to get probabilities\n",
    "    probabilities = torch.sigmoid(output)\n",
    "    predictions.extend(probabilities.cpu().detach().numpy())  # Convert to numpy if not already\n",
    "\n",
    "# Convert the lists to PyTorch tensors for convenience\n",
    "targets_tensor = torch.tensor(targets)\n",
    "predictions_tensor = torch.tensor(predictions)\n",
    "\n",
    "# Plot histograms for target and predicted distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(targets_tensor.numpy(), bins='auto', color='blue', alpha=0.7)\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Target Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_tensor.numpy(), bins=20, color='orange', alpha=0.7)\n",
    "plt.title('Prediction Distribution')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the lists to NumPy arrays for convenience\n",
    "targets_np = targets_tensor.numpy()\n",
    "predictions_np = predictions_tensor.numpy()\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr_ann, tpr_ann, thresholds = roc_curve(targets_np, predictions_np)\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(targets_np, predictions_np)\n",
    "\n",
    "# Visualize ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_ann, tpr_ann, color='red', lw=2, label=f'ANN ROC (AUC = {roc_auc:.3f})')\n",
    "plt.plot(fpr_lstm, tpr_lstm, color='blue', lw=2, label=f'LSTM ROC (AUC = {0.854:.3f})')\n",
    "plt.plot(fpr_tcn, tpr_tcn, color='orange', lw=2, label=f'TCN ROC (AUC = {0.835:.3f})')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Validation Set ROC Curve - ANN, TCN, LSTM')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Calculate balanced accuracy with a threshold of 0.9\n",
    "threshold = 0.93\n",
    "binary_predictions = (predictions_np > threshold).astype(int)\n",
    "balanced_acc = balanced_accuracy_score(targets_np, binary_predictions)\n",
    "\n",
    "print(f'Balanced Accuracy with threshold {threshold}: {balanced_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model seems to have reached the best performance at around 20th epoch\n",
    "plot_loss_curve(\"csv_logs_all/lightning_logs/version_2\", model_name='ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shap values for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAL SET LSTM\n",
    "# Load the pre-trained model from the pickle file\n",
    "model_path = 'models_nn_all/sepsis_lstm.pkl'  # Provide the path to pre-trained model\n",
    "with open(model_path, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Add a sigmoid layer on top of the model\n",
    "sigmoid_layer = nn.Sigmoid()\n",
    "modified_model = nn.Sequential(loaded_model, sigmoid_layer)\n",
    "\n",
    "# Assuming DataLoader is named 'test_dataloader'\n",
    "# Replace this with the actual DataLoader you have\n",
    "val_dataloader = dm.val_dataloader()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "modified_model.eval()\n",
    "\n",
    "# Concatenate entire val_dataloader into a single batch\n",
    "all_samples = []\n",
    "all_targets = []\n",
    "\n",
    "for batch_data, batch_targets in val_dataloader:\n",
    "    all_samples.append(batch_data)\n",
    "    all_targets.append(batch_targets)\n",
    "\n",
    "sample_data = torch.cat(all_samples[:2000], dim=0)\n",
    "targets = torch.cat(all_targets[:2000], dim=0)\n",
    "print(sample_data.size())\n",
    "\n",
    "# Randomly select 100 samples\n",
    "num_samples_to_select = 2000\n",
    "random_indices = torch.randperm(sample_data.size(0))[:num_samples_to_select]\n",
    "sample_data_selected = sample_data[random_indices]\n",
    "print(sample_data_selected.size())\n",
    "\n",
    "# samples for background\n",
    "num_bsamples_to_select = 100\n",
    "background_indices = torch.randperm(sample_data.size(0))[:num_bsamples_to_select]\n",
    "background_data_selected = sample_data[random_indices]\n",
    "\n",
    "# Move data to CPU if the model is on GPU\n",
    "sample_data_selected = sample_data_selected.to('cpu')  # Modify if needed\n",
    "background_data_selected  = background_data_selected .to('cpu')\n",
    "modified_model = modified_model.to('cpu')\n",
    "\n",
    "# Create the SHAP explainer\n",
    "explainer = shap.DeepExplainer(modified_model, data=background_data_selected .detach())\n",
    "\n",
    "# Calculate SHAP values for the sample data\n",
    "shap_values = explainer.shap_values(sample_data_selected.detach(), check_additivity=False)\n",
    "\n",
    "print(shap_values.shape, sample_data_selected.numpy().shape)\n",
    "\n",
    "shap_values = np.sum(shap_values, axis=1)\n",
    "sample_data_selected = np.sum(sample_data_selected.numpy(), axis=1)\n",
    "\n",
    "# Plot the SHAP values\n",
    "feature_names = dm.columns_to_keep\n",
    "shap.summary_plot(shap_values, sample_data_selected, feature_names=feature_names, show=False, title = \"Shapley values on Validation Set (LSTM\")\n",
    "plt.title(\"Shapley Values on Validation Set (LSTM)\", fontsize = 16)\n",
    "plt.xlim((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values for the sample data was : \n",
    "explainer.expected_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAL SET LSTM\n",
    "# Load the pre-trained model from the pickle file\n",
    "model_path = 'models_nn_all/sepsis_tcn.pkl'  # Provide the path to pre-trained model\n",
    "with open(model_path, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Add a sigmoid layer on top of the model\n",
    "sigmoid_layer = nn.Sigmoid()\n",
    "modified_model = nn.Sequential(loaded_model, sigmoid_layer)\n",
    "\n",
    "# Assuming DataLoader is named 'test_dataloader'\n",
    "# Replace this with the actual DataLoader you have\n",
    "val_dataloader = dm.val_dataloader()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "modified_model.eval()\n",
    "\n",
    "# Concatenate entire val_dataloader into a single batch\n",
    "all_samples = []\n",
    "all_targets = []\n",
    "\n",
    "for batch_data, batch_targets in val_dataloader:\n",
    "    all_samples.append(batch_data)\n",
    "    all_targets.append(batch_targets)\n",
    "\n",
    "sample_data = torch.cat(all_samples[:2000], dim=0)\n",
    "targets = torch.cat(all_targets[:10000], dim=0)\n",
    "print(sample_data.size())\n",
    "\n",
    "# Randomly select 100 samples\n",
    "num_samples_to_select = 2000\n",
    "random_indices = torch.randperm(sample_data.size(0))[:num_samples_to_select]\n",
    "sample_data_selected = sample_data[random_indices]\n",
    "print(sample_data_selected.size())\n",
    "\n",
    "# samples for background\n",
    "num_bsamples_to_select = 100\n",
    "background_indices = torch.randperm(sample_data.size(0))[:num_bsamples_to_select]\n",
    "background_data_selected = sample_data[random_indices]\n",
    "\n",
    "# Move data to GPU if the model is on GPU\n",
    "sample_data_selected = sample_data_selected.to('cpu')  # Modify if needed\n",
    "background_data_selected = background_data_selected.to('cpu') \n",
    "modified_model = modified_model.to('cpu')\n",
    "\n",
    "# Create the SHAP explainer\n",
    "explainer = shap.DeepExplainer(modified_model, data=background_data_selected.detach())\n",
    "\n",
    "# Calculate SHAP values for the sample data\n",
    "shap_values = explainer.shap_values(sample_data_selected.detach(), check_additivity=False)\n",
    "print(shap_values.shape, sample_data_selected.numpy().shape)\n",
    "\n",
    "shap_values = np.sum(shap_values, axis=1)\n",
    "sample_data_selected = np.sum(sample_data_selected.numpy(), axis=1)\n",
    "\n",
    "# Plot the SHAP values\n",
    "feature_names = dm.columns_to_keep\n",
    "shap.summary_plot(shap_values, sample_data_selected, feature_names=feature_names, show=False, title = \"Shapley Values on validation sample (TCN)\")\n",
    "plt.title(\"Shapley Values on Validation Set (TCN)\", fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.expected_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values, sample_data_selected, feature_names=feature_names)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jNAJyEkT2ifX",
    "KbQ3ZaWk2iff",
    "0Cq7lp892iff"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
